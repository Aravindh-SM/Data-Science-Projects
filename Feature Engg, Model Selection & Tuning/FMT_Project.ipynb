{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-07-19 11:55:00</td>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-07-19 12:32:00</td>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>...</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-07-19 13:17:00</td>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>...</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-07-19 14:43:00</td>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>...</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-07-19 15:22:00</td>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2008-10-16 15:13:00</td>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>100.0</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>1.3424</td>\n",
       "      <td>...</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>2008-10-16 20:49:00</td>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>1.4333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2008-10-17 05:26:00</td>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2008-10-17 06:01:00</td>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>1.4622</td>\n",
       "      <td>...</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2008-10-17 06:07:00</td>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>100.0</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 592 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Time        0        1          2          3       4  \\\n",
       "0     2008-07-19 11:55:00  3030.93  2564.00  2187.7333  1411.1265  1.3602   \n",
       "1     2008-07-19 12:32:00  3095.78  2465.14  2230.4222  1463.6606  0.8294   \n",
       "2     2008-07-19 13:17:00  2932.61  2559.94  2186.4111  1698.0172  1.5102   \n",
       "3     2008-07-19 14:43:00  2988.72  2479.90  2199.0333   909.7926  1.3204   \n",
       "4     2008-07-19 15:22:00  3032.24  2502.87  2233.3667  1326.5200  1.5334   \n",
       "...                   ...      ...      ...        ...        ...     ...   \n",
       "1562  2008-10-16 15:13:00  2899.41  2464.36  2179.7333  3085.3781  1.4843   \n",
       "1563  2008-10-16 20:49:00  3052.31  2522.55  2198.5667  1124.6595  0.8763   \n",
       "1564  2008-10-17 05:26:00  2978.81  2379.78  2206.3000  1110.4967  0.8236   \n",
       "1565  2008-10-17 06:01:00  2894.92  2532.01  2177.0333  1183.7287  1.5726   \n",
       "1566  2008-10-17 06:07:00  2944.92  2450.76  2195.4444  2914.1792  1.5978   \n",
       "\n",
       "          5         6       7       8  ...       581     582     583     584  \\\n",
       "0     100.0   97.6133  0.1242  1.5005  ...       NaN  0.5005  0.0118  0.0035   \n",
       "1     100.0  102.3433  0.1247  1.4966  ...  208.2045  0.5019  0.0223  0.0055   \n",
       "2     100.0   95.4878  0.1241  1.4436  ...   82.8602  0.4958  0.0157  0.0039   \n",
       "3     100.0  104.2367  0.1217  1.4882  ...   73.8432  0.4990  0.0103  0.0025   \n",
       "4     100.0  100.3967  0.1235  1.5031  ...       NaN  0.4800  0.4766  0.1045   \n",
       "...     ...       ...     ...     ...  ...       ...     ...     ...     ...   \n",
       "1562  100.0   82.2467  0.1248  1.3424  ...  203.1720  0.4988  0.0143  0.0039   \n",
       "1563  100.0   98.4689  0.1205  1.4333  ...       NaN  0.4975  0.0131  0.0036   \n",
       "1564  100.0   99.4122  0.1208     NaN  ...   43.5231  0.4987  0.0153  0.0041   \n",
       "1565  100.0   98.7978  0.1213  1.4622  ...   93.4941  0.5004  0.0178  0.0038   \n",
       "1566  100.0   85.1011  0.1235     NaN  ...  137.7844  0.4987  0.0181  0.0040   \n",
       "\n",
       "          585     586     587     588       589  Pass/Fail  \n",
       "0      2.3630     NaN     NaN     NaN       NaN         -1  \n",
       "1      4.4447  0.0096  0.0201  0.0060  208.2045         -1  \n",
       "2      3.1745  0.0584  0.0484  0.0148   82.8602          1  \n",
       "3      2.0544  0.0202  0.0149  0.0044   73.8432         -1  \n",
       "4     99.3032  0.0202  0.0149  0.0044   73.8432         -1  \n",
       "...       ...     ...     ...     ...       ...        ...  \n",
       "1562   2.8669  0.0068  0.0138  0.0047  203.1720         -1  \n",
       "1563   2.6238  0.0068  0.0138  0.0047  203.1720         -1  \n",
       "1564   3.0590  0.0197  0.0086  0.0025   43.5231         -1  \n",
       "1565   3.5662  0.0262  0.0245  0.0075   93.4941         -1  \n",
       "1566   3.6275  0.0117  0.0162  0.0045  137.7844         -1  \n",
       "\n",
       "[1567 rows x 592 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import and explore the data\n",
    "s=pd.read_csv('signal-data.csv')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time          0\n",
       "0             6\n",
       "1             7\n",
       "2            14\n",
       "3            14\n",
       "             ..\n",
       "586           1\n",
       "587           1\n",
       "588           1\n",
       "589           1\n",
       "Pass/Fail     0\n",
       "Length: 592, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Data cleansing:\n",
    "s.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>...</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1          2          3       4      5         6       7  \\\n",
       "0  3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   97.6133  0.1242   \n",
       "1  3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0  102.3433  0.1247   \n",
       "2  2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   95.4878  0.1241   \n",
       "3  2988.72  2479.90  2199.0333   909.7926  1.3204  100.0  104.2367  0.1217   \n",
       "4  3032.24  2502.87  2233.3667  1326.5200  1.5334  100.0  100.3967  0.1235   \n",
       "\n",
       "        8       9  ...       581     582     583     584      585     586  \\\n",
       "0  1.5005  0.0162  ...       NaN  0.5005  0.0118  0.0035   2.3630     NaN   \n",
       "1  1.4966 -0.0005  ...  208.2045  0.5019  0.0223  0.0055   4.4447  0.0096   \n",
       "2  1.4436  0.0041  ...   82.8602  0.4958  0.0157  0.0039   3.1745  0.0584   \n",
       "3  1.4882 -0.0124  ...   73.8432  0.4990  0.0103  0.0025   2.0544  0.0202   \n",
       "4  1.5031 -0.0031  ...       NaN  0.4800  0.4766  0.1045  99.3032  0.0202   \n",
       "\n",
       "      587     588       589  Pass  \n",
       "0     NaN     NaN       NaN     0  \n",
       "1  0.0201  0.0060  208.2045     0  \n",
       "2  0.0484  0.0148   82.8602     1  \n",
       "3  0.0149  0.0044   73.8432     0  \n",
       "4  0.0149  0.0044   73.8432     0  \n",
       "\n",
       "[5 rows x 591 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time stamp data is not required, hence we can drop it\n",
    "s.drop('Time',1,inplace=True)\n",
    "\n",
    "# Pass/Fail column can be renamed for clarity\n",
    "s.replace({-1: 0},inplace=True)\n",
    "s.rename(columns={\"Pass/Fail\": \"Pass\",},inplace=True)\n",
    "# Pass == 0, means product passed, else failed\n",
    "s1=s.copy()\n",
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41951"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total number of missing values\n",
    "s.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1429"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Maximum number of missing values in a single column\n",
    "s.isnull().sum().max() #No. of rows is just 1567, hence 1426 is very high for single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27287032160>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#100 columns with max missing values have just 10 distinct number of missing values\n",
    "sns.countplot(x=s.isnull().sum().sort_values()[-100:][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.382897\n",
       "1       0.446713\n",
       "2       0.893427\n",
       "3       0.893427\n",
       "4       0.893427\n",
       "          ...   \n",
       "586     0.063816\n",
       "587     0.063816\n",
       "588     0.063816\n",
       "589     0.063816\n",
       "Pass    0.000000\n",
       "Length: 591, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=s.isna().sum()*100/s.shape[0]\n",
    "df #Percentage of missing values in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x272870204c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=df[df>10].sort_values()\n",
    "plt.figure(figsize=(15,9))\n",
    "sns.barplot(x=df.index,y=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ind=df[df>30].index.tolist()\n",
    "len(df_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.01, 0.0]     258\n",
       "(0.0, 0.1]        61\n",
       "(0.1, 0.5]        18\n",
       "(0.5, 1.0]         9\n",
       "(1.0, 10.0]       55\n",
       "(10.0, 50.0]      50\n",
       "(50.0, 500.0]     58\n",
       "(500.0, inf]      82\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.cut(s.var().round(2),[-0.01,0,0.1,0.5,1,10,50,500,float('inf')]).value_counts().sort_index()\n",
    "#There are many features with a very few unique values even though there are 1567 values per feature.\n",
    "#Features with such low variance doesn't contribute much to the model learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping features with variance less than 0.1 and those with more than 30% missing values\n",
    "\n",
    "df = s.drop('Pass',1).var().round(2)\n",
    "low_v = df[df<=0.1].index.tolist()\n",
    "\n",
    "# combine the list and remove them frm the main dataset\n",
    "s.drop(np.unique(low_v + df_ind).tolist(),1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>12</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>568</th>\n",
       "      <th>569</th>\n",
       "      <th>570</th>\n",
       "      <th>572</th>\n",
       "      <th>574</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>585</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>202.4396</td>\n",
       "      <td>7.9558</td>\n",
       "      <td>414.8710</td>\n",
       "      <td>10.0433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>533.8500</td>\n",
       "      <td>8.95</td>\n",
       "      <td>3.0624</td>\n",
       "      <td>1.6765</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>200.5470</td>\n",
       "      <td>10.1548</td>\n",
       "      <td>414.7347</td>\n",
       "      <td>9.2599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>535.0164</td>\n",
       "      <td>5.92</td>\n",
       "      <td>2.0111</td>\n",
       "      <td>1.1065</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>202.0179</td>\n",
       "      <td>9.5157</td>\n",
       "      <td>416.7075</td>\n",
       "      <td>9.3144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4119</td>\n",
       "      <td>68.8489</td>\n",
       "      <td>535.0245</td>\n",
       "      <td>11.21</td>\n",
       "      <td>4.0923</td>\n",
       "      <td>2.0952</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>201.8482</td>\n",
       "      <td>9.6052</td>\n",
       "      <td>422.2894</td>\n",
       "      <td>9.6924</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7290</td>\n",
       "      <td>25.0363</td>\n",
       "      <td>530.5682</td>\n",
       "      <td>9.33</td>\n",
       "      <td>2.8971</td>\n",
       "      <td>1.7585</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>201.9424</td>\n",
       "      <td>10.5661</td>\n",
       "      <td>420.5925</td>\n",
       "      <td>10.3387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>532.0155</td>\n",
       "      <td>8.83</td>\n",
       "      <td>3.1776</td>\n",
       "      <td>1.6597</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>203.9867</td>\n",
       "      <td>11.7692</td>\n",
       "      <td>419.3404</td>\n",
       "      <td>10.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8844</td>\n",
       "      <td>15.4662</td>\n",
       "      <td>536.3418</td>\n",
       "      <td>7.98</td>\n",
       "      <td>2.6401</td>\n",
       "      <td>1.4879</td>\n",
       "      <td>11.7256</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>204.0173</td>\n",
       "      <td>9.1620</td>\n",
       "      <td>405.8178</td>\n",
       "      <td>10.2285</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7089</td>\n",
       "      <td>20.9118</td>\n",
       "      <td>537.9264</td>\n",
       "      <td>5.48</td>\n",
       "      <td>1.9077</td>\n",
       "      <td>1.0187</td>\n",
       "      <td>17.8379</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.3197</td>\n",
       "      <td>29.0954</td>\n",
       "      <td>530.3709</td>\n",
       "      <td>6.49</td>\n",
       "      <td>2.1760</td>\n",
       "      <td>1.2237</td>\n",
       "      <td>17.7267</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>197.2448</td>\n",
       "      <td>9.7354</td>\n",
       "      <td>401.9153</td>\n",
       "      <td>9.8630</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8844</td>\n",
       "      <td>15.4662</td>\n",
       "      <td>534.3936</td>\n",
       "      <td>9.13</td>\n",
       "      <td>3.2524</td>\n",
       "      <td>1.7085</td>\n",
       "      <td>19.2104</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2639</td>\n",
       "      <td>21.1128</td>\n",
       "      <td>528.7918</td>\n",
       "      <td>6.81</td>\n",
       "      <td>2.2727</td>\n",
       "      <td>1.2878</td>\n",
       "      <td>22.9183</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0        1          2          3       4         6        12  \\\n",
       "0     3030.93  2564.00  2187.7333  1411.1265  1.3602   97.6133  202.4396   \n",
       "1     3095.78  2465.14  2230.4222  1463.6606  0.8294  102.3433  200.5470   \n",
       "2     2932.61  2559.94  2186.4111  1698.0172  1.5102   95.4878  202.0179   \n",
       "3     2988.72  2479.90  2199.0333   909.7926  1.3204  104.2367  201.8482   \n",
       "4     3032.24  2502.87  2233.3667  1326.5200  1.5334  100.3967  201.9424   \n",
       "...       ...      ...        ...        ...     ...       ...       ...   \n",
       "1562  2899.41  2464.36  2179.7333  3085.3781  1.4843   82.2467  203.9867   \n",
       "1563  3052.31  2522.55  2198.5667  1124.6595  0.8763   98.4689  204.0173   \n",
       "1564  2978.81  2379.78  2206.3000  1110.4967  0.8236   99.4122    0.0000   \n",
       "1565  2894.92  2532.01  2177.0333  1183.7287  1.5726   98.7978  197.2448   \n",
       "1566  2944.92  2450.76  2195.4444  2914.1792  1.5978   85.1011    0.0000   \n",
       "\n",
       "           14        15       16  ...     568      569       570    572  \\\n",
       "0      7.9558  414.8710  10.0433  ...  0.0000   0.0000  533.8500   8.95   \n",
       "1     10.1548  414.7347   9.2599  ...  0.0000   0.0000  535.0164   5.92   \n",
       "2      9.5157  416.7075   9.3144  ...  0.4119  68.8489  535.0245  11.21   \n",
       "3      9.6052  422.2894   9.6924  ...  2.7290  25.0363  530.5682   9.33   \n",
       "4     10.5661  420.5925  10.3387  ...  0.0000   0.0000  532.0155   8.83   \n",
       "...       ...       ...      ...  ...     ...      ...       ...    ...   \n",
       "1562  11.7692  419.3404  10.2397  ...  1.8844  15.4662  536.3418   7.98   \n",
       "1563   9.1620  405.8178  10.2285  ...  1.7089  20.9118  537.9264   5.48   \n",
       "1564   0.0000    0.0000   0.0000  ...  4.3197  29.0954  530.3709   6.49   \n",
       "1565   9.7354  401.9153   9.8630  ...  1.8844  15.4662  534.3936   9.13   \n",
       "1566   0.0000    0.0000   0.0000  ...  3.2639  21.1128  528.7918   6.81   \n",
       "\n",
       "         574     576      577      585       589  Pass  \n",
       "0     3.0624  1.6765  14.9509   2.3630    0.0000     0  \n",
       "1     2.0111  1.1065  10.9003   4.4447  208.2045     0  \n",
       "2     4.0923  2.0952   9.2721   3.1745   82.8602     1  \n",
       "3     2.8971  1.7585   8.5831   2.0544   73.8432     0  \n",
       "4     3.1776  1.6597  10.9698  99.3032   73.8432     0  \n",
       "...      ...     ...      ...      ...       ...   ...  \n",
       "1562  2.6401  1.4879  11.7256   2.8669  203.1720     0  \n",
       "1563  1.9077  1.0187  17.8379   2.6238  203.1720     0  \n",
       "1564  2.1760  1.2237  17.7267   3.0590   43.5231     0  \n",
       "1565  3.2524  1.7085  19.2104   3.5662   93.4941     0  \n",
       "1566  2.2727  1.2878  22.9183   3.6275  137.7844     0  \n",
       "\n",
       "[1567 rows x 255 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replacing Nan values with zero, since it is highly likely the the signal value 0 is parsed as Nan\n",
    "s.replace(np.NaN,0,inplace=True)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0]        0\n",
       "(1.0, 10.0]      79\n",
       "(10.0, 50.0]     39\n",
       "(50.0, 500.0]    54\n",
       "(500.0, inf]     82\n",
       "Name: VIF, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = s.drop('Pass',1)\n",
    "vif = pd.Series(np.linalg.inv(df.corr().values).diagonal(),index=df.columns,\n",
    "          name='VIF').abs().sort_values(ascending=False).round(2)\n",
    "df = pd.cut(vif.round(1),[0,1,10,50,500,float('inf')]).value_counts().sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>6</th>\n",
       "      <th>12</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>18</th>\n",
       "      <th>21</th>\n",
       "      <th>...</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>568</th>\n",
       "      <th>569</th>\n",
       "      <th>570</th>\n",
       "      <th>574</th>\n",
       "      <th>577</th>\n",
       "      <th>585</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>202.4396</td>\n",
       "      <td>7.9558</td>\n",
       "      <td>414.8710</td>\n",
       "      <td>192.3963</td>\n",
       "      <td>-5419.00</td>\n",
       "      <td>...</td>\n",
       "      <td>42.3877</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>533.8500</td>\n",
       "      <td>3.0624</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>200.5470</td>\n",
       "      <td>10.1548</td>\n",
       "      <td>414.7347</td>\n",
       "      <td>191.2872</td>\n",
       "      <td>-5441.50</td>\n",
       "      <td>...</td>\n",
       "      <td>18.1087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>535.0164</td>\n",
       "      <td>2.0111</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>202.0179</td>\n",
       "      <td>9.5157</td>\n",
       "      <td>416.7075</td>\n",
       "      <td>192.7035</td>\n",
       "      <td>-5447.75</td>\n",
       "      <td>...</td>\n",
       "      <td>24.7524</td>\n",
       "      <td>267.064</td>\n",
       "      <td>0.4119</td>\n",
       "      <td>68.8489</td>\n",
       "      <td>535.0245</td>\n",
       "      <td>4.0923</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>201.8482</td>\n",
       "      <td>9.6052</td>\n",
       "      <td>422.2894</td>\n",
       "      <td>192.1557</td>\n",
       "      <td>-5468.25</td>\n",
       "      <td>...</td>\n",
       "      <td>62.7572</td>\n",
       "      <td>268.228</td>\n",
       "      <td>2.7290</td>\n",
       "      <td>25.0363</td>\n",
       "      <td>530.5682</td>\n",
       "      <td>2.8971</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>201.9424</td>\n",
       "      <td>10.5661</td>\n",
       "      <td>420.5925</td>\n",
       "      <td>191.6037</td>\n",
       "      <td>-5476.25</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>532.0155</td>\n",
       "      <td>3.1776</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>203.9867</td>\n",
       "      <td>11.7692</td>\n",
       "      <td>419.3404</td>\n",
       "      <td>193.7470</td>\n",
       "      <td>-5418.75</td>\n",
       "      <td>...</td>\n",
       "      <td>32.3812</td>\n",
       "      <td>264.272</td>\n",
       "      <td>1.8844</td>\n",
       "      <td>15.4662</td>\n",
       "      <td>536.3418</td>\n",
       "      <td>2.6401</td>\n",
       "      <td>11.7256</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>204.0173</td>\n",
       "      <td>9.1620</td>\n",
       "      <td>405.8178</td>\n",
       "      <td>193.7889</td>\n",
       "      <td>-6408.75</td>\n",
       "      <td>...</td>\n",
       "      <td>32.1048</td>\n",
       "      <td>266.832</td>\n",
       "      <td>1.7089</td>\n",
       "      <td>20.9118</td>\n",
       "      <td>537.9264</td>\n",
       "      <td>1.9077</td>\n",
       "      <td>17.8379</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-5153.25</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0316</td>\n",
       "      <td>256.730</td>\n",
       "      <td>4.3197</td>\n",
       "      <td>29.0954</td>\n",
       "      <td>530.3709</td>\n",
       "      <td>2.1760</td>\n",
       "      <td>17.7267</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>197.2448</td>\n",
       "      <td>9.7354</td>\n",
       "      <td>401.9153</td>\n",
       "      <td>187.3818</td>\n",
       "      <td>-5271.75</td>\n",
       "      <td>...</td>\n",
       "      <td>18.9966</td>\n",
       "      <td>264.272</td>\n",
       "      <td>1.8844</td>\n",
       "      <td>15.4662</td>\n",
       "      <td>534.3936</td>\n",
       "      <td>3.2524</td>\n",
       "      <td>19.2104</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-5319.50</td>\n",
       "      <td>...</td>\n",
       "      <td>21.4914</td>\n",
       "      <td>257.974</td>\n",
       "      <td>3.2639</td>\n",
       "      <td>21.1128</td>\n",
       "      <td>528.7918</td>\n",
       "      <td>2.2727</td>\n",
       "      <td>22.9183</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0        1          2          3         6        12       14  \\\n",
       "0     3030.93  2564.00  2187.7333  1411.1265   97.6133  202.4396   7.9558   \n",
       "1     3095.78  2465.14  2230.4222  1463.6606  102.3433  200.5470  10.1548   \n",
       "2     2932.61  2559.94  2186.4111  1698.0172   95.4878  202.0179   9.5157   \n",
       "3     2988.72  2479.90  2199.0333   909.7926  104.2367  201.8482   9.6052   \n",
       "4     3032.24  2502.87  2233.3667  1326.5200  100.3967  201.9424  10.5661   \n",
       "...       ...      ...        ...        ...       ...       ...      ...   \n",
       "1562  2899.41  2464.36  2179.7333  3085.3781   82.2467  203.9867  11.7692   \n",
       "1563  3052.31  2522.55  2198.5667  1124.6595   98.4689  204.0173   9.1620   \n",
       "1564  2978.81  2379.78  2206.3000  1110.4967   99.4122    0.0000   0.0000   \n",
       "1565  2894.92  2532.01  2177.0333  1183.7287   98.7978  197.2448   9.7354   \n",
       "1566  2944.92  2450.76  2195.4444  2914.1792   85.1011    0.0000   0.0000   \n",
       "\n",
       "            15        18       21  ...      561      562     568      569  \\\n",
       "0     414.8710  192.3963 -5419.00  ...  42.3877    0.000  0.0000   0.0000   \n",
       "1     414.7347  191.2872 -5441.50  ...  18.1087    0.000  0.0000   0.0000   \n",
       "2     416.7075  192.7035 -5447.75  ...  24.7524  267.064  0.4119  68.8489   \n",
       "3     422.2894  192.1557 -5468.25  ...  62.7572  268.228  2.7290  25.0363   \n",
       "4     420.5925  191.6037 -5476.25  ...  22.0500    0.000  0.0000   0.0000   \n",
       "...        ...       ...      ...  ...      ...      ...     ...      ...   \n",
       "1562  419.3404  193.7470 -5418.75  ...  32.3812  264.272  1.8844  15.4662   \n",
       "1563  405.8178  193.7889 -6408.75  ...  32.1048  266.832  1.7089  20.9118   \n",
       "1564    0.0000    0.0000 -5153.25  ...  13.0316  256.730  4.3197  29.0954   \n",
       "1565  401.9153  187.3818 -5271.75  ...  18.9966  264.272  1.8844  15.4662   \n",
       "1566    0.0000    0.0000 -5319.50  ...  21.4914  257.974  3.2639  21.1128   \n",
       "\n",
       "           570     574      577      585       589  Pass  \n",
       "0     533.8500  3.0624  14.9509   2.3630    0.0000     0  \n",
       "1     535.0164  2.0111  10.9003   4.4447  208.2045     0  \n",
       "2     535.0245  4.0923   9.2721   3.1745   82.8602     1  \n",
       "3     530.5682  2.8971   8.5831   2.0544   73.8432     0  \n",
       "4     532.0155  3.1776  10.9698  99.3032   73.8432     0  \n",
       "...        ...     ...      ...      ...       ...   ...  \n",
       "1562  536.3418  2.6401  11.7256   2.8669  203.1720     0  \n",
       "1563  537.9264  1.9077  17.8379   2.6238  203.1720     0  \n",
       "1564  530.3709  2.1760  17.7267   3.0590   43.5231     0  \n",
       "1565  534.3936  3.2524  19.2104   3.5662   93.4941     0  \n",
       "1566  528.7918  2.2727  22.9183   3.6275  137.7844     0  \n",
       "\n",
       "[1567 rows x 146 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are several highly multicollinear features. Generally vif>10 is considered as high, hence such features could be removed\n",
    "\n",
    "#Below function is to keep removing the highest vif feature one-by-one, until the highest vif is less than 10\n",
    "def capture_vif(df):\n",
    "    high_vif = []\n",
    "    while 1:\n",
    "        temp_vif = pd.Series(np.linalg.inv(df.corr().values).diagonal(),index=df.columns,name='VIF').abs().sort_values(ascending=False).round(2)\n",
    "        maxi = temp_vif.max()\n",
    "        if maxi>10:\n",
    "            high_vif = temp_vif[temp_vif == maxi].index.tolist()[0]\n",
    "            df.drop(high_vif,1,inplace=True)\n",
    "        else:\n",
    "            return df\n",
    "    return\n",
    "\n",
    "#Removing features with vif>10\n",
    "s = capture_vif(s)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347     99.936184\n",
       "521     98.659860\n",
       "Pass    93.363114\n",
       "500     58.136567\n",
       "499     54.499043\n",
       "511     52.456924\n",
       "545     47.925973\n",
       "419     45.564773\n",
       "486     33.694959\n",
       "418     32.801532\n",
       "482     32.291002\n",
       "568     25.271219\n",
       "569     25.271219\n",
       "562     25.271219\n",
       "487     23.484365\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for features that has dominant single value \n",
    "\n",
    "df = s.apply(pd.value_counts).max()*100/s.shape[0]\n",
    "df.sort_values(ascending=False,inplace=True)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_0</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_12</th>\n",
       "      <th>Feature_14</th>\n",
       "      <th>Feature_15</th>\n",
       "      <th>Feature_18</th>\n",
       "      <th>Feature_21</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_561</th>\n",
       "      <th>Feature_562</th>\n",
       "      <th>Feature_568</th>\n",
       "      <th>Feature_569</th>\n",
       "      <th>Feature_570</th>\n",
       "      <th>Feature_574</th>\n",
       "      <th>Feature_577</th>\n",
       "      <th>Feature_585</th>\n",
       "      <th>Feature_589</th>\n",
       "      <th>Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>202.4396</td>\n",
       "      <td>7.9558</td>\n",
       "      <td>414.8710</td>\n",
       "      <td>192.3963</td>\n",
       "      <td>-5419.00</td>\n",
       "      <td>...</td>\n",
       "      <td>42.3877</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>533.8500</td>\n",
       "      <td>3.0624</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>200.5470</td>\n",
       "      <td>10.1548</td>\n",
       "      <td>414.7347</td>\n",
       "      <td>191.2872</td>\n",
       "      <td>-5441.50</td>\n",
       "      <td>...</td>\n",
       "      <td>18.1087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>535.0164</td>\n",
       "      <td>2.0111</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>202.0179</td>\n",
       "      <td>9.5157</td>\n",
       "      <td>416.7075</td>\n",
       "      <td>192.7035</td>\n",
       "      <td>-5447.75</td>\n",
       "      <td>...</td>\n",
       "      <td>24.7524</td>\n",
       "      <td>267.064</td>\n",
       "      <td>0.4119</td>\n",
       "      <td>68.8489</td>\n",
       "      <td>535.0245</td>\n",
       "      <td>4.0923</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>201.8482</td>\n",
       "      <td>9.6052</td>\n",
       "      <td>422.2894</td>\n",
       "      <td>192.1557</td>\n",
       "      <td>-5468.25</td>\n",
       "      <td>...</td>\n",
       "      <td>62.7572</td>\n",
       "      <td>268.228</td>\n",
       "      <td>2.7290</td>\n",
       "      <td>25.0363</td>\n",
       "      <td>530.5682</td>\n",
       "      <td>2.8971</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>201.9424</td>\n",
       "      <td>10.5661</td>\n",
       "      <td>420.5925</td>\n",
       "      <td>191.6037</td>\n",
       "      <td>-5476.25</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>532.0155</td>\n",
       "      <td>3.1776</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>203.9867</td>\n",
       "      <td>11.7692</td>\n",
       "      <td>419.3404</td>\n",
       "      <td>193.7470</td>\n",
       "      <td>-5418.75</td>\n",
       "      <td>...</td>\n",
       "      <td>32.3812</td>\n",
       "      <td>264.272</td>\n",
       "      <td>1.8844</td>\n",
       "      <td>15.4662</td>\n",
       "      <td>536.3418</td>\n",
       "      <td>2.6401</td>\n",
       "      <td>11.7256</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>204.0173</td>\n",
       "      <td>9.1620</td>\n",
       "      <td>405.8178</td>\n",
       "      <td>193.7889</td>\n",
       "      <td>-6408.75</td>\n",
       "      <td>...</td>\n",
       "      <td>32.1048</td>\n",
       "      <td>266.832</td>\n",
       "      <td>1.7089</td>\n",
       "      <td>20.9118</td>\n",
       "      <td>537.9264</td>\n",
       "      <td>1.9077</td>\n",
       "      <td>17.8379</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-5153.25</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0316</td>\n",
       "      <td>256.730</td>\n",
       "      <td>4.3197</td>\n",
       "      <td>29.0954</td>\n",
       "      <td>530.3709</td>\n",
       "      <td>2.1760</td>\n",
       "      <td>17.7267</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>197.2448</td>\n",
       "      <td>9.7354</td>\n",
       "      <td>401.9153</td>\n",
       "      <td>187.3818</td>\n",
       "      <td>-5271.75</td>\n",
       "      <td>...</td>\n",
       "      <td>18.9966</td>\n",
       "      <td>264.272</td>\n",
       "      <td>1.8844</td>\n",
       "      <td>15.4662</td>\n",
       "      <td>534.3936</td>\n",
       "      <td>3.2524</td>\n",
       "      <td>19.2104</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-5319.50</td>\n",
       "      <td>...</td>\n",
       "      <td>21.4914</td>\n",
       "      <td>257.974</td>\n",
       "      <td>3.2639</td>\n",
       "      <td>21.1128</td>\n",
       "      <td>528.7918</td>\n",
       "      <td>2.2727</td>\n",
       "      <td>22.9183</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature_0  Feature_1  Feature_2  Feature_3  Feature_6  Feature_12  \\\n",
       "0       3030.93    2564.00  2187.7333  1411.1265    97.6133    202.4396   \n",
       "1       3095.78    2465.14  2230.4222  1463.6606   102.3433    200.5470   \n",
       "2       2932.61    2559.94  2186.4111  1698.0172    95.4878    202.0179   \n",
       "3       2988.72    2479.90  2199.0333   909.7926   104.2367    201.8482   \n",
       "4       3032.24    2502.87  2233.3667  1326.5200   100.3967    201.9424   \n",
       "...         ...        ...        ...        ...        ...         ...   \n",
       "1562    2899.41    2464.36  2179.7333  3085.3781    82.2467    203.9867   \n",
       "1563    3052.31    2522.55  2198.5667  1124.6595    98.4689    204.0173   \n",
       "1564    2978.81    2379.78  2206.3000  1110.4967    99.4122      0.0000   \n",
       "1565    2894.92    2532.01  2177.0333  1183.7287    98.7978    197.2448   \n",
       "1566    2944.92    2450.76  2195.4444  2914.1792    85.1011      0.0000   \n",
       "\n",
       "      Feature_14  Feature_15  Feature_18  Feature_21  ...  Feature_561  \\\n",
       "0         7.9558    414.8710    192.3963    -5419.00  ...      42.3877   \n",
       "1        10.1548    414.7347    191.2872    -5441.50  ...      18.1087   \n",
       "2         9.5157    416.7075    192.7035    -5447.75  ...      24.7524   \n",
       "3         9.6052    422.2894    192.1557    -5468.25  ...      62.7572   \n",
       "4        10.5661    420.5925    191.6037    -5476.25  ...      22.0500   \n",
       "...          ...         ...         ...         ...  ...          ...   \n",
       "1562     11.7692    419.3404    193.7470    -5418.75  ...      32.3812   \n",
       "1563      9.1620    405.8178    193.7889    -6408.75  ...      32.1048   \n",
       "1564      0.0000      0.0000      0.0000    -5153.25  ...      13.0316   \n",
       "1565      9.7354    401.9153    187.3818    -5271.75  ...      18.9966   \n",
       "1566      0.0000      0.0000      0.0000    -5319.50  ...      21.4914   \n",
       "\n",
       "      Feature_562  Feature_568  Feature_569  Feature_570  Feature_574  \\\n",
       "0           0.000       0.0000       0.0000     533.8500       3.0624   \n",
       "1           0.000       0.0000       0.0000     535.0164       2.0111   \n",
       "2         267.064       0.4119      68.8489     535.0245       4.0923   \n",
       "3         268.228       2.7290      25.0363     530.5682       2.8971   \n",
       "4           0.000       0.0000       0.0000     532.0155       3.1776   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1562      264.272       1.8844      15.4662     536.3418       2.6401   \n",
       "1563      266.832       1.7089      20.9118     537.9264       1.9077   \n",
       "1564      256.730       4.3197      29.0954     530.3709       2.1760   \n",
       "1565      264.272       1.8844      15.4662     534.3936       3.2524   \n",
       "1566      257.974       3.2639      21.1128     528.7918       2.2727   \n",
       "\n",
       "      Feature_577  Feature_585  Feature_589  Pass  \n",
       "0         14.9509       2.3630       0.0000     0  \n",
       "1         10.9003       4.4447     208.2045     0  \n",
       "2          9.2721       3.1745      82.8602     1  \n",
       "3          8.5831       2.0544      73.8432     0  \n",
       "4         10.9698      99.3032      73.8432     0  \n",
       "...           ...          ...          ...   ...  \n",
       "1562      11.7256       2.8669     203.1720     0  \n",
       "1563      17.8379       2.6238     203.1720     0  \n",
       "1564      17.7267       3.0590      43.5231     0  \n",
       "1565      19.2104       3.5662      93.4941     0  \n",
       "1566      22.9183       3.6275     137.7844     0  \n",
       "\n",
       "[1567 rows x 146 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=s.columns.to_list()\n",
    "cols.pop()\n",
    "cols = ['Feature_'+str(i) for i in cols]\n",
    "cols.append('Pass')\n",
    "s.columns=cols\n",
    "s\n",
    "# Renamed columns for better handling and avoiding confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.drop(columns=['Feature_347','Feature_521'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([356, 313, 279, 269, 262, 238, 222, 214, 206, 201, 199, 195, 179,\n",
       "       156, 151, 144, 140, 140, 133, 133])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(s.drop('Pass',1).columns)\n",
    "out=[]\n",
    "for i in cols:\n",
    "    Q1,Q3=np.quantile(s[i],0.25),np.quantile(s[i],0.75)\n",
    "    IQR=Q3-Q1\n",
    "    L,U=Q1-1.5*IQR,Q3+1.5*IQR\n",
    "    out.append(len([t for t in s[i] if t<L or t>U]))\n",
    "out.sort(reverse=True)\n",
    "np.array(out[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x272890d51c0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see too many features have many outliers  \n",
    "Hence dropping those features is not good preprocessing  \n",
    "Manipulating outliers with median value is not as good as using transformation in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "###Before transformation, the target variable is dropped here###\n",
    "\n",
    "X=s.drop('Pass',1)\n",
    "Y=s.Pass\n",
    "\n",
    "qt = QuantileTransformer(output_distribution='normal')\n",
    "X = pd.DataFrame(qt.fit_transform(X),columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2728b99f460>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out=[]\n",
    "for i in list(X.columns):\n",
    "    Q1,Q3=np.quantile(X[i],0.25),np.quantile(X[i],0.75)\n",
    "    IQR=Q3-Q1\n",
    "    L,U=Q1-1.5*IQR,Q3+1.5*IQR\n",
    "    out.append(len([t for t in X[i] if t<L or t>U]))\n",
    "out.sort(reverse=True)\n",
    "plt.plot(out) \n",
    "\n",
    "#Clearly the outliers are handled in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data analysis & visualisation:\n",
    "\n",
    "Since there are too many features, visualization for statistical analysis will need too much computing, hence these visualiztions are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_0</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_12</th>\n",
       "      <th>Feature_14</th>\n",
       "      <th>Feature_15</th>\n",
       "      <th>Feature_18</th>\n",
       "      <th>Feature_21</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_557</th>\n",
       "      <th>Feature_561</th>\n",
       "      <th>Feature_562</th>\n",
       "      <th>Feature_568</th>\n",
       "      <th>Feature_569</th>\n",
       "      <th>Feature_570</th>\n",
       "      <th>Feature_574</th>\n",
       "      <th>Feature_577</th>\n",
       "      <th>Feature_585</th>\n",
       "      <th>Feature_589</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.007169</td>\n",
       "      <td>-0.008753</td>\n",
       "      <td>-0.018434</td>\n",
       "      <td>-0.023136</td>\n",
       "      <td>-0.018639</td>\n",
       "      <td>-0.001343</td>\n",
       "      <td>-0.002762</td>\n",
       "      <td>-0.002130</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.623788</td>\n",
       "      <td>-0.002032</td>\n",
       "      <td>-0.648239</td>\n",
       "      <td>-0.728923</td>\n",
       "      <td>-0.646224</td>\n",
       "      <td>-0.000293</td>\n",
       "      <td>-0.000319</td>\n",
       "      <td>0.015461</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>-0.056565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.041437</td>\n",
       "      <td>1.047673</td>\n",
       "      <td>1.100678</td>\n",
       "      <td>1.096603</td>\n",
       "      <td>1.100425</td>\n",
       "      <td>1.017248</td>\n",
       "      <td>1.023509</td>\n",
       "      <td>1.024809</td>\n",
       "      <td>1.023247</td>\n",
       "      <td>1.025330</td>\n",
       "      <td>...</td>\n",
       "      <td>2.165125</td>\n",
       "      <td>1.013700</td>\n",
       "      <td>2.209045</td>\n",
       "      <td>2.195955</td>\n",
       "      <td>2.209929</td>\n",
       "      <td>1.014011</td>\n",
       "      <td>1.011373</td>\n",
       "      <td>1.071939</td>\n",
       "      <td>1.014671</td>\n",
       "      <td>1.210760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.673702</td>\n",
       "      <td>-0.673957</td>\n",
       "      <td>-0.676854</td>\n",
       "      <td>-0.672942</td>\n",
       "      <td>-0.678433</td>\n",
       "      <td>-0.674377</td>\n",
       "      <td>-0.674530</td>\n",
       "      <td>-0.674670</td>\n",
       "      <td>-0.674146</td>\n",
       "      <td>-0.674276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674138</td>\n",
       "      <td>-0.676584</td>\n",
       "      <td>-0.674937</td>\n",
       "      <td>-0.673799</td>\n",
       "      <td>-0.673882</td>\n",
       "      <td>-0.669195</td>\n",
       "      <td>-0.674677</td>\n",
       "      <td>-0.675182</td>\n",
       "      <td>-0.674979</td>\n",
       "      <td>-0.667419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000741</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000772</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>-0.000627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003764</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.266584</td>\n",
       "      <td>-0.487893</td>\n",
       "      <td>-0.171458</td>\n",
       "      <td>-0.000677</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>-0.001233</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>-0.003764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.674423</td>\n",
       "      <td>0.674590</td>\n",
       "      <td>0.681594</td>\n",
       "      <td>0.673702</td>\n",
       "      <td>0.673702</td>\n",
       "      <td>0.673850</td>\n",
       "      <td>0.674880</td>\n",
       "      <td>0.674955</td>\n",
       "      <td>0.674148</td>\n",
       "      <td>0.674816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574595</td>\n",
       "      <td>0.676854</td>\n",
       "      <td>0.674788</td>\n",
       "      <td>0.674723</td>\n",
       "      <td>0.674617</td>\n",
       "      <td>0.676854</td>\n",
       "      <td>0.673702</td>\n",
       "      <td>0.672129</td>\n",
       "      <td>0.674668</td>\n",
       "      <td>0.672129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>...</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Feature_0    Feature_1    Feature_2    Feature_3    Feature_6  \\\n",
       "count  1567.000000  1567.000000  1567.000000  1567.000000  1567.000000   \n",
       "mean     -0.007169    -0.008753    -0.018434    -0.023136    -0.018639   \n",
       "std       1.041437     1.047673     1.100678     1.096603     1.100425   \n",
       "min      -5.199338    -5.199338    -5.199338    -5.199338    -5.199338   \n",
       "25%      -0.673702    -0.673957    -0.676854    -0.672942    -0.678433   \n",
       "50%      -0.000741     0.000046     0.001255     0.002509     0.000000   \n",
       "75%       0.674423     0.674590     0.681594     0.673702     0.673702   \n",
       "max       5.199338     5.199338     5.199338     5.199338     5.199338   \n",
       "\n",
       "        Feature_12   Feature_14   Feature_15   Feature_18   Feature_21  ...  \\\n",
       "count  1567.000000  1567.000000  1567.000000  1567.000000  1567.000000  ...   \n",
       "mean     -0.001343    -0.002762    -0.002130    -0.002667     0.002098  ...   \n",
       "std       1.017248     1.023509     1.024809     1.023247     1.025330  ...   \n",
       "min      -5.199338    -5.199338    -5.199338    -5.199338    -5.199338  ...   \n",
       "25%      -0.674377    -0.674530    -0.674670    -0.674146    -0.674276  ...   \n",
       "50%      -0.000772     0.000118     0.000475     0.000920    -0.000627  ...   \n",
       "75%       0.673850     0.674880     0.674955     0.674148     0.674816  ...   \n",
       "max       5.199338     5.199338     5.199338     5.199338     5.199338  ...   \n",
       "\n",
       "       Feature_557  Feature_561  Feature_562  Feature_568  Feature_569  \\\n",
       "count  1567.000000  1567.000000  1567.000000  1567.000000  1567.000000   \n",
       "mean     -0.623788    -0.002032    -0.648239    -0.728923    -0.646224   \n",
       "std       2.165125     1.013700     2.209045     2.195955     2.209929   \n",
       "min      -5.199338    -5.199338    -5.199338    -5.199338    -5.199338   \n",
       "25%      -0.674138    -0.676584    -0.674937    -0.673799    -0.673882   \n",
       "50%      -0.003764     0.000702     0.266584    -0.487893    -0.171458   \n",
       "75%       0.574595     0.676854     0.674788     0.674723     0.674617   \n",
       "max       5.199338     5.199338     5.199338     5.199338     5.199338   \n",
       "\n",
       "       Feature_570  Feature_574  Feature_577  Feature_585  Feature_589  \n",
       "count  1567.000000  1567.000000  1567.000000  1567.000000  1567.000000  \n",
       "mean     -0.000293    -0.000319     0.015461     0.000675    -0.056565  \n",
       "std       1.014011     1.011373     1.071939     1.014671     1.210760  \n",
       "min      -5.199338    -5.199338    -5.199338    -5.199338    -5.199338  \n",
       "25%      -0.669195    -0.674677    -0.675182    -0.674979    -0.667419  \n",
       "50%      -0.000677     0.000323    -0.001233     0.001154    -0.003764  \n",
       "75%       0.676854     0.673702     0.672129     0.674668     0.672129  \n",
       "max       5.199338     5.199338     5.199338     5.199338     5.199338  \n",
       "\n",
       "[8 rows x 143 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2728ba502e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(Y)\n",
    "\n",
    "#There is high imbalance in Target variable since passed values are very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_0</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_12</th>\n",
       "      <th>Feature_14</th>\n",
       "      <th>Feature_15</th>\n",
       "      <th>Feature_18</th>\n",
       "      <th>Feature_21</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_557</th>\n",
       "      <th>Feature_561</th>\n",
       "      <th>Feature_562</th>\n",
       "      <th>Feature_568</th>\n",
       "      <th>Feature_569</th>\n",
       "      <th>Feature_570</th>\n",
       "      <th>Feature_574</th>\n",
       "      <th>Feature_577</th>\n",
       "      <th>Feature_585</th>\n",
       "      <th>Feature_589</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Feature_0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.148459</td>\n",
       "      <td>0.022716</td>\n",
       "      <td>0.017161</td>\n",
       "      <td>0.011075</td>\n",
       "      <td>0.020689</td>\n",
       "      <td>-0.009614</td>\n",
       "      <td>-0.006225</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>-0.017635</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031211</td>\n",
       "      <td>0.061686</td>\n",
       "      <td>0.011174</td>\n",
       "      <td>0.028881</td>\n",
       "      <td>0.023973</td>\n",
       "      <td>-0.084218</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>-0.011583</td>\n",
       "      <td>-0.000796</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature_1</th>\n",
       "      <td>-0.148459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023721</td>\n",
       "      <td>-0.037142</td>\n",
       "      <td>-0.039981</td>\n",
       "      <td>0.036433</td>\n",
       "      <td>-0.020876</td>\n",
       "      <td>-0.012253</td>\n",
       "      <td>0.041619</td>\n",
       "      <td>0.036723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>-0.046864</td>\n",
       "      <td>0.009201</td>\n",
       "      <td>-0.004362</td>\n",
       "      <td>-0.004979</td>\n",
       "      <td>0.028197</td>\n",
       "      <td>0.029658</td>\n",
       "      <td>-0.031056</td>\n",
       "      <td>0.021035</td>\n",
       "      <td>0.011915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature_2</th>\n",
       "      <td>0.022716</td>\n",
       "      <td>-0.023721</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.505115</td>\n",
       "      <td>0.067849</td>\n",
       "      <td>-0.015809</td>\n",
       "      <td>-0.011392</td>\n",
       "      <td>-0.012423</td>\n",
       "      <td>-0.007428</td>\n",
       "      <td>0.036965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065714</td>\n",
       "      <td>0.024517</td>\n",
       "      <td>-0.023175</td>\n",
       "      <td>-0.011977</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>-0.067617</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>-0.060806</td>\n",
       "      <td>-0.014698</td>\n",
       "      <td>-0.029678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature_3</th>\n",
       "      <td>0.017161</td>\n",
       "      <td>-0.037142</td>\n",
       "      <td>0.505115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302363</td>\n",
       "      <td>-0.046907</td>\n",
       "      <td>-0.060917</td>\n",
       "      <td>-0.052655</td>\n",
       "      <td>-0.043969</td>\n",
       "      <td>0.014260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019183</td>\n",
       "      <td>0.036003</td>\n",
       "      <td>0.046988</td>\n",
       "      <td>0.031010</td>\n",
       "      <td>0.052429</td>\n",
       "      <td>-0.028505</td>\n",
       "      <td>0.059454</td>\n",
       "      <td>0.019393</td>\n",
       "      <td>-0.020452</td>\n",
       "      <td>-0.065485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature_6</th>\n",
       "      <td>0.011075</td>\n",
       "      <td>-0.039981</td>\n",
       "      <td>0.067849</td>\n",
       "      <td>-0.302363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>0.044433</td>\n",
       "      <td>0.021546</td>\n",
       "      <td>0.025714</td>\n",
       "      <td>-0.018349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020044</td>\n",
       "      <td>-0.047341</td>\n",
       "      <td>-0.106406</td>\n",
       "      <td>-0.088257</td>\n",
       "      <td>-0.107424</td>\n",
       "      <td>0.090537</td>\n",
       "      <td>-0.039476</td>\n",
       "      <td>-0.038635</td>\n",
       "      <td>-0.009770</td>\n",
       "      <td>-0.022854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature_570</th>\n",
       "      <td>-0.084218</td>\n",
       "      <td>0.028197</td>\n",
       "      <td>-0.067617</td>\n",
       "      <td>-0.028505</td>\n",
       "      <td>0.090537</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.057948</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>-0.003619</td>\n",
       "      <td>-0.118340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068143</td>\n",
       "      <td>0.029713</td>\n",
       "      <td>-0.081272</td>\n",
       "      <td>-0.063967</td>\n",
       "      <td>-0.081244</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.299086</td>\n",
       "      <td>-0.187410</td>\n",
       "      <td>0.065151</td>\n",
       "      <td>0.027322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature_574</th>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.029658</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.059454</td>\n",
       "      <td>-0.039476</td>\n",
       "      <td>-0.002939</td>\n",
       "      <td>-0.062399</td>\n",
       "      <td>-0.045919</td>\n",
       "      <td>-0.001092</td>\n",
       "      <td>0.041410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031288</td>\n",
       "      <td>-0.000972</td>\n",
       "      <td>-0.035914</td>\n",
       "      <td>-0.023662</td>\n",
       "      <td>-0.030720</td>\n",
       "      <td>-0.299086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.241305</td>\n",
       "      <td>-0.031967</td>\n",
       "      <td>-0.011604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature_577</th>\n",
       "      <td>-0.011583</td>\n",
       "      <td>-0.031056</td>\n",
       "      <td>-0.060806</td>\n",
       "      <td>0.019393</td>\n",
       "      <td>-0.038635</td>\n",
       "      <td>-0.016400</td>\n",
       "      <td>0.020581</td>\n",
       "      <td>-0.023080</td>\n",
       "      <td>-0.005406</td>\n",
       "      <td>0.057305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>-0.053639</td>\n",
       "      <td>0.054030</td>\n",
       "      <td>0.061373</td>\n",
       "      <td>0.055266</td>\n",
       "      <td>-0.187410</td>\n",
       "      <td>0.241305</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.035537</td>\n",
       "      <td>-0.015310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature_585</th>\n",
       "      <td>-0.000796</td>\n",
       "      <td>0.021035</td>\n",
       "      <td>-0.014698</td>\n",
       "      <td>-0.020452</td>\n",
       "      <td>-0.009770</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>0.006915</td>\n",
       "      <td>0.033271</td>\n",
       "      <td>-0.000842</td>\n",
       "      <td>-0.042871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>-0.007037</td>\n",
       "      <td>-0.037262</td>\n",
       "      <td>-0.030062</td>\n",
       "      <td>-0.036490</td>\n",
       "      <td>0.065151</td>\n",
       "      <td>-0.031967</td>\n",
       "      <td>-0.035537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature_589</th>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.011915</td>\n",
       "      <td>-0.029678</td>\n",
       "      <td>-0.065485</td>\n",
       "      <td>-0.022854</td>\n",
       "      <td>-0.032178</td>\n",
       "      <td>0.056793</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>-0.011631</td>\n",
       "      <td>-0.038359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031438</td>\n",
       "      <td>0.026881</td>\n",
       "      <td>0.049288</td>\n",
       "      <td>0.064155</td>\n",
       "      <td>0.044098</td>\n",
       "      <td>0.027322</td>\n",
       "      <td>-0.011604</td>\n",
       "      <td>-0.015310</td>\n",
       "      <td>0.038027</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature_0  Feature_1  Feature_2  Feature_3  Feature_6  \\\n",
       "Feature_0     1.000000  -0.148459   0.022716   0.017161   0.011075   \n",
       "Feature_1    -0.148459   1.000000  -0.023721  -0.037142  -0.039981   \n",
       "Feature_2     0.022716  -0.023721   1.000000   0.505115   0.067849   \n",
       "Feature_3     0.017161  -0.037142   0.505115   1.000000  -0.302363   \n",
       "Feature_6     0.011075  -0.039981   0.067849  -0.302363   1.000000   \n",
       "...                ...        ...        ...        ...        ...   \n",
       "Feature_570  -0.084218   0.028197  -0.067617  -0.028505   0.090537   \n",
       "Feature_574   0.021056   0.029658   0.000763   0.059454  -0.039476   \n",
       "Feature_577  -0.011583  -0.031056  -0.060806   0.019393  -0.038635   \n",
       "Feature_585  -0.000796   0.021035  -0.014698  -0.020452  -0.009770   \n",
       "Feature_589   0.000064   0.011915  -0.029678  -0.065485  -0.022854   \n",
       "\n",
       "             Feature_12  Feature_14  Feature_15  Feature_18  Feature_21  ...  \\\n",
       "Feature_0      0.020689   -0.009614   -0.006225    0.035200   -0.017635  ...   \n",
       "Feature_1      0.036433   -0.020876   -0.012253    0.041619    0.036723  ...   \n",
       "Feature_2     -0.015809   -0.011392   -0.012423   -0.007428    0.036965  ...   \n",
       "Feature_3     -0.046907   -0.060917   -0.052655   -0.043969    0.014260  ...   \n",
       "Feature_6      0.050870    0.044433    0.021546    0.025714   -0.018349  ...   \n",
       "...                 ...         ...         ...         ...         ...  ...   \n",
       "Feature_570    0.008894    0.057948    0.081967   -0.003619   -0.118340  ...   \n",
       "Feature_574   -0.002939   -0.062399   -0.045919   -0.001092    0.041410  ...   \n",
       "Feature_577   -0.016400    0.020581   -0.023080   -0.005406    0.057305  ...   \n",
       "Feature_585   -0.002566    0.006915    0.033271   -0.000842   -0.042871  ...   \n",
       "Feature_589   -0.032178    0.056793    0.002827   -0.011631   -0.038359  ...   \n",
       "\n",
       "             Feature_557  Feature_561  Feature_562  Feature_568  Feature_569  \\\n",
       "Feature_0      -0.031211     0.061686     0.011174     0.028881     0.023973   \n",
       "Feature_1       0.011154    -0.046864     0.009201    -0.004362    -0.004979   \n",
       "Feature_2       0.065714     0.024517    -0.023175    -0.011977     0.000355   \n",
       "Feature_3       0.019183     0.036003     0.046988     0.031010     0.052429   \n",
       "Feature_6      -0.020044    -0.047341    -0.106406    -0.088257    -0.107424   \n",
       "...                  ...          ...          ...          ...          ...   \n",
       "Feature_570    -0.068143     0.029713    -0.081272    -0.063967    -0.081244   \n",
       "Feature_574    -0.031288    -0.000972    -0.035914    -0.023662    -0.030720   \n",
       "Feature_577     0.002931    -0.053639     0.054030     0.061373     0.055266   \n",
       "Feature_585     0.002927    -0.007037    -0.037262    -0.030062    -0.036490   \n",
       "Feature_589     0.031438     0.026881     0.049288     0.064155     0.044098   \n",
       "\n",
       "             Feature_570  Feature_574  Feature_577  Feature_585  Feature_589  \n",
       "Feature_0      -0.084218     0.021056    -0.011583    -0.000796     0.000064  \n",
       "Feature_1       0.028197     0.029658    -0.031056     0.021035     0.011915  \n",
       "Feature_2      -0.067617     0.000763    -0.060806    -0.014698    -0.029678  \n",
       "Feature_3      -0.028505     0.059454     0.019393    -0.020452    -0.065485  \n",
       "Feature_6       0.090537    -0.039476    -0.038635    -0.009770    -0.022854  \n",
       "...                  ...          ...          ...          ...          ...  \n",
       "Feature_570     1.000000    -0.299086    -0.187410     0.065151     0.027322  \n",
       "Feature_574    -0.299086     1.000000     0.241305    -0.031967    -0.011604  \n",
       "Feature_577    -0.187410     0.241305     1.000000    -0.035537    -0.015310  \n",
       "Feature_585     0.065151    -0.031967    -0.035537     1.000000     0.038027  \n",
       "Feature_589     0.027322    -0.011604    -0.015310     0.038027     1.000000  \n",
       "\n",
       "[143 rows x 143 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Outliers are handled,hence univariate analysis is not required\n",
    "\n",
    "X.corr() #Correlation is already handled, hence doesn't need pairplot or heatmap visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data pre-processing:  \n",
    "(i) Features are already segregated from target variable  \n",
    "(ii) Target imbalance found previously should be handled  and f1_score should be used for metrics.  \n",
    "(iii) Since QuantileTransformer is already performed on dataset, standardization is not required again. Anyway the final evaluation is going to be done from dataset unseen by the current transformed dataset, hence transforming test and train dataset  together is not an issue here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1463\n",
       "1     104\n",
       "Name: Pass, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_normal_indices = np.random.choice(1463, size = 104, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.75      , 0.59375   , 0.84375   , 0.9375    , 0.78125   ,\n",
       "       0.875     , 0.8125    , 0.6875    , 0.90625   , 0.84375   ,\n",
       "       0.78125   , 0.84375   , 0.9375    , 0.9375    , 0.9375    ,\n",
       "       0.90625   , 0.96875   , 1.        , 0.87096774, 1.        ,\n",
       "       1.        , 1.        , 0.96774194, 0.93548387, 1.        ,\n",
       "       0.93548387, 0.87096774, 0.90322581, 0.96774194, 0.90322581,\n",
       "       1.        , 1.        , 0.96774194, 0.96774194, 1.        ,\n",
       "       1.        , 0.93548387, 0.93548387, 0.96774194, 0.83870968,\n",
       "       0.96774194, 0.93548387, 0.80645161, 0.87096774, 0.93548387,\n",
       "       0.96774194, 0.90322581, 1.        , 0.93548387, 1.        ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross validation\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "kfold = KFold(n_splits=50)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[440,   0],\n",
       "       [ 31,   0]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. Model training, testing and tuning:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=20)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred=rf.predict(X_test)\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97       440\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.93       471\n",
      "   macro avg       0.47      0.50      0.48       471\n",
      "weighted avg       0.87      0.93      0.90       471\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\aravindhsm\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred)) #Error in predicting Pass==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on train: 1.0\n",
      "Model Accuracy on test: 0.9278131634819533\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC(C= 4, kernel='rbf', gamma='scale')\n",
    "svc_model.fit(X_train, y_train)\n",
    "sv_train_predict = svc_model .predict(X_train)\n",
    "print(f\"Model Accuracy on train: {accuracy_score(y_train, sv_train_predict)}\")\n",
    "sv_test_predict = svc_model .predict(X_test)\n",
    "print(f\"Model Accuracy on test: {accuracy_score(y_test, sv_test_predict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       440\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.93       471\n",
      "   macro avg       0.47      0.50      0.48       471\n",
      "weighted avg       0.87      0.93      0.90       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,sv_test_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
